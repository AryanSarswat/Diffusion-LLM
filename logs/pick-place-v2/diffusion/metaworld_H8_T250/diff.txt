diff --git a/collect_metaworld2.py b/collect_metaworld2.py
index a878423..70e79bc 100644
--- a/collect_metaworld2.py
+++ b/collect_metaworld2.py
@@ -5,14 +5,16 @@ import pickle
 import imageio
 from tqdm import tqdm
 import argparse
+from gymnasium.spaces import Box
 import random
 import os
 
+os.environ['MUJOCO_GL'] = 'egl'
 
 def collect_metaworld_data(env_name, num_trajectories, max_path_length, save_path, render_videos=False, video_path=None, video_fps=30, video_dim=(256, 256)):
     # Initialize Metaworld environment
-    mt = metaworld.ML1(env_name)
-    env = mt.train_classes[env_name]()
+    mt = metaworld.MT1(env_name)
+    env = mt.train_classes[env_name](render_mode='rgb_array')
     task = random.choice(mt.train_tasks)
     env.set_task(task)
 
@@ -24,6 +26,7 @@ def collect_metaworld_data(env_name, num_trajectories, max_path_length, save_pat
         'timeouts': [],
         'success': [],
     }
+
     policy_map = {
         'drawer-close-v2': SawyerDrawerCloseV2Policy(),
         'door-open-v2': SawyerDoorOpenV2Policy(),
@@ -33,6 +36,8 @@ def collect_metaworld_data(env_name, num_trajectories, max_path_length, save_pat
         'button-press-wall-v2': SawyerButtonPressWallV2Policy(),
         'push-v2': SawyerPushV2Policy(),
         'hand-insert-v2': SawyerHandInsertV2Policy(),
+        'pick-place-v2' : SawyerPickPlaceV2Policy(),
+        'pick-place-wall-v2' : SawyerPickPlaceWallV2Policy(),
         # Add other mappings here
     }
 
@@ -46,21 +51,26 @@ def collect_metaworld_data(env_name, num_trajectories, max_path_length, save_pat
         os.makedirs(video_path, exist_ok=True)
 
     for traj_idx in tqdm(range(num_trajectories), desc=f"Collecting trajectories for {env_name}"):
-        obs, _ = env.reset()
         observations = []
         actions = []
         rewards = []
         terminals = []
         timeouts = []
         success = False
+        timeout = False
         done = False
 
         # Set up video frames list if rendering is enabled
         frames = []
 
+        env = mt.train_classes[env_name](render_mode='rgb_array')
+        task = random.choice(mt.train_tasks)
+        env.set_task(task)
+        obs, _ = env.reset()
+        
         for t in range(max_path_length):
             action = policy.get_action(obs)
-            next_obs, reward, _, _,info = env.step(action)
+            next_obs, reward, _, _, info = env.step(action)
             done = int(info['success']) == 1
             observations.append(obs)
             actions.append(action)
@@ -69,25 +79,29 @@ def collect_metaworld_data(env_name, num_trajectories, max_path_length, save_pat
             obs = next_obs
 
             # Render and store frames if enabled
-            if render_videos:
-                img = env.render(render_mode="offscreen")
+            if render_videos and (traj_idx % 100 == 0):
+                img = env.render()
                 frames.append(img)
 
             if done:
                 if info.get('success', False):  # Check if the task was successful
                     success = True
+                else:
+                    timeout = True
                 break
-
+                
+            timeouts.append(timeout)
+        
         # Save trajectory data
         data['observations'].append(np.array(observations))
         data['actions'].append(np.array(actions))
         data['rewards'].append(np.array(rewards))
         data['terminals'].append(np.array(terminals))
-        data['timeouts'].append(np.array(timeouts))
+        data['timeouts'].append(np.array([timeout]))
         data['success'].append(success)
 
         # Save video if enabled
-        if render_videos and video_path is not None:
+        if render_videos and (video_path is not None) and (traj_idx % 100 == 0):
             video_file = os.path.join(video_path, f'TEST_trajectory_{traj_idx}.mp4')
             imageio.mimwrite(video_file, frames, fps=video_fps)
             print(f"Saved video to {video_file}")
@@ -102,7 +116,7 @@ if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     parser.add_argument('--env_name', type=str, default='button-press-wall-v2')
     parser.add_argument('--num_trajectories', type=int, default=1000)
-    parser.add_argument('--max_path_length', type=int, default=500)
+    parser.add_argument('--max_path_length', type=int, default=250)
     parser.add_argument('--save_path', type=str, default='metaworld_drawer_close_data2.pkl')
     parser.add_argument('--render_videos', action='store_true', help="Enable video rendering")
     parser.add_argument('--video_path', type=str, default='videos', help="Directory to save rendered videos")
diff --git a/config/locomotion.py b/config/locomotion.py
index ac2bbd9..39ea670 100644
--- a/config/locomotion.py
+++ b/config/locomotion.py
@@ -500,13 +500,13 @@ pick_place_v2= {
         'model': 'models.TemporalUnet',
         'diffusion': 'models.GaussianDiffusion',
         'horizon': 8,  # Adjust based on your planning horizon
-        'n_diffusion_steps': 20,
+        'n_diffusion_steps': 250,
         'action_weight': 1,
         'loss_weights': None,
         'loss_discount': 1,
         'predict_epsilon': False,
         'dim_mults': (1, 2, 4, 8),
-        'attention': False,
+        'attention': True,
         'renderer': 'utils.MetaworldRenderer',
 
         # Dataset parameters
@@ -515,7 +515,7 @@ pick_place_v2= {
         'preprocess_fns': [],
         'clip_denoised': False,
         'use_padding': True,
-        'max_path_length': 500,  # Match with data collection max_path_length
+        'max_path_length': 250,  # Match with data collection max_path_length
 
         # Serialization
         'logbase': logbase,
@@ -523,10 +523,10 @@ pick_place_v2= {
         'exp_name': watch(args_to_watch),
 
         # Training parameters
-        'n_steps_per_epoch': 290,
+        'n_steps_per_epoch': 500,
         'loss_type': 'l1',
         'n_train_steps': 426000,
-        'batch_size': 32,
+        'batch_size': 512,
         'learning_rate': 2e-4,
         'gradient_accumulate_every': 1,
         'ema_decay': 0.995,
@@ -558,7 +558,7 @@ pick_place_v2= {
         'preprocess_fns': [],
         'clip_denoised': False,
         'use_padding': True,
-        'max_path_length': 500,
+        'max_path_length': 250,
     },
 }
 
@@ -627,4 +627,4 @@ push_v2= {
         'use_padding': True,
         'max_path_length': 500,
     },
-}
+}
\ No newline at end of file
diff --git a/diffuser/datasets/buffer.py b/diffuser/datasets/buffer.py
index da23142..e44dbbf 100644
--- a/diffuser/datasets/buffer.py
+++ b/diffuser/datasets/buffer.py
@@ -59,7 +59,7 @@ class ReplayBuffer:
         dim = array.shape[-1]
         shape = (self.max_n_episodes, self.max_path_length, dim)
         self._dict[key] = np.zeros(shape, dtype=np.float32)
-        # print(f'[ utils/mujoco ] Allocated {key} with size {shape}')
+        print(f'[ utils/mujoco ] Allocated {key} with size {shape}')
 
     def add_path(self, path):
         path_length = len(path['observations'])
diff --git a/diffuser/datasets/metaworld_sequence.py b/diffuser/datasets/metaworld_sequence.py
index 7b04ae4..d19d44a 100644
--- a/diffuser/datasets/metaworld_sequence.py
+++ b/diffuser/datasets/metaworld_sequence.py
@@ -16,9 +16,9 @@ class MetaworldSequenceDataset(SequenceDataset):
                  normalizer='LimitsNormalizer', preprocess_fns=[], max_path_length=1000,
                  max_n_episodes=10000, termination_penalty=0, use_padding=True, seed=None):
         self.preprocess_fn = get_preprocess_fn(preprocess_fns, env)
-        mt = metaworld.ML1(env)
+        mt = metaworld.MT1(env)
         env = mt.train_classes[env]()
-        task = random.choice(mt.test_tasks)
+        task = random.choice(mt.train_tasks)
         env.set_task(task)
         self.env = env
         self.horizon = horizon
diff --git a/diffuser/models/helpers.py b/diffuser/models/helpers.py
index 6485219..01fecf9 100644
--- a/diffuser/models/helpers.py
+++ b/diffuser/models/helpers.py
@@ -164,7 +164,9 @@ class WeightedLoss(nn.Module):
         loss = self._loss(pred, targ)
         weighted_loss = (loss * self.weights).mean()
         a0_loss = (loss[:, 0, :self.action_dim] / self.weights[0, :self.action_dim]).mean()
-        return weighted_loss, {'a0_loss': a0_loss}
+        a04_loss = (loss[:, :3, :self.action_dim] / self.weights[:3, :self.action_dim]).mean()
+        a4__loss = (loss[:, 4:, :self.action_dim] / self.weights[4:8, :self.action_dim]).mean()
+        return weighted_loss, {'a0_loss': a0_loss, 'a04_loss': a04_loss, 'a4-_loss': a4__loss}
 
 class ValueLoss(nn.Module):
     def __init__(self, *args):
diff --git a/diffuser/utils/serialization.py b/diffuser/utils/serialization.py
index d219472..21b7b4c 100644
--- a/diffuser/utils/serialization.py
+++ b/diffuser/utils/serialization.py
@@ -34,6 +34,7 @@ def load_config(*loadpath):
     return config
 
 def load_diffusion(*loadpath, epoch='latest', device='cuda:0', seed=None):
+    print(loadpath)
     dataset_config = load_config(*loadpath, 'dataset_config.pkl')
     render_config = load_config(*loadpath, 'render_config.pkl')
     model_config = load_config(*loadpath, 'model_config.pkl')
diff --git a/metaworld_button_press_data_150_uniform_noisy.pkl b/metaworld_button_press_data_150_uniform_noisy.pkl
deleted file mode 100644
index 0ea89ab..0000000
Binary files a/metaworld_button_press_data_150_uniform_noisy.pkl and /dev/null differ
diff --git a/metaworld_button_press_validation.pkl b/metaworld_button_press_validation.pkl
deleted file mode 100644
index bc42a9b..0000000
Binary files a/metaworld_button_press_validation.pkl and /dev/null differ
diff --git a/metaworld_door_open_data_100_uniform.pkl b/metaworld_door_open_data_100_uniform.pkl
deleted file mode 100644
index 9d324d7..0000000
Binary files a/metaworld_door_open_data_100_uniform.pkl and /dev/null differ
diff --git a/metaworld_drawer_close_data2.pkl b/metaworld_drawer_close_data2.pkl
deleted file mode 100644
index 8b545c7..0000000
Binary files a/metaworld_drawer_close_data2.pkl and /dev/null differ
diff --git a/scripts/plan_guided_lfn.py b/scripts/plan_guided_lfn.py
index 950cec6..25db4d6 100644
--- a/scripts/plan_guided_lfn.py
+++ b/scripts/plan_guided_lfn.py
@@ -7,6 +7,7 @@ import imageio
 import os
 import torch
 import numpy as np
+from tqdm import tqdm
 
 #-----------------------------------------------------------------------------#
 #----------------------------------- setup -----------------------------------#
@@ -296,6 +297,7 @@ def predefined_loss_fn2(x, obs_dim, action_dim, normalizer,
     return loss
 
 
+
 def _loss_fn (x, obs_dim,action_dim,normalizer ):
     actions = x[:, :, :action_dim]
     actions = actions * torch.tensor(normalizer.normalizers['actions'].stds, device=x.device) + torch.tensor(normalizer.normalizers['actions'].means, device=x.device)
@@ -315,8 +317,105 @@ def _loss_fn (x, obs_dim,action_dim,normalizer ):
 
     return loss_per_trajectory  # Shape: [batch_size]
 
+def pick_place_wall_loss_fn(x, obs_dim, action_dim, normalizer,
+                          wall_pos=[0.0, 0.77, 0.25],  # Wall midpoint from environment
+                          wall_dims=[0.5, 0.02, 0.5],  # Approximate wall dimensions
+                          min_safe_dist=0.05,
+                          delta_t=1.0,
+                          return_diagnostics=False):
+    """
+    Loss function for pick-place task with wall obstacle avoidance.
+    Penalizes:
+    1. Trajectories that come too close to the wall
+    2. Trajectories that don't successfully grasp the object
+    3. Trajectories that don't move the object to the target
+    
+    Args:
+        x (torch.Tensor): Input tensor of shape [batch_size, horizon, transition_dim]
+        obs_dim (int): Observation dimension offset in x
+        action_dim (int): Action dimension in x
+        normalizer (object): Normalizer with means and stds for actions/observations
+        wall_pos (list): Center position of the wall [x, y, z]
+        wall_dims (list): Full dimensions of the wall [width, thickness, height]
+        min_safe_dist (float): Minimum safe distance from wall
+        delta_t (float): Time step for action integration
+        return_diagnostics (bool): Whether to return detailed diagnostic information
+    
+    Returns:
+        torch.Tensor: Combined loss of shape [batch_size]
+        (Optional) dict: Diagnostic information if return_diagnostics is True
+    """
+    import torch
+    
+    # Convert parameters to tensors
+    wall_pos = torch.tensor(wall_pos, device=x.device, dtype=x.dtype).view(1, 1, 3)
+    wall_half_size = torch.tensor(wall_dims, device=x.device, dtype=x.dtype).view(1, 1, 3) / 2
+
+    # Split and unnormalize actions and observations
+    actions = x[:, :, :action_dim]
+    obs = x[:, :, action_dim:]
+
+    # Unnormalize
+    action_stds = torch.from_numpy(normalizer.normalizers['actions'].stds).to(x.device).to(x.dtype).view(1, 1, -1)
+    action_means = torch.from_numpy(normalizer.normalizers['actions'].means).to(x.device).to(x.dtype).view(1, 1, -1)
+    actions = actions * action_stds + action_means
+
+    obs_stds = torch.from_numpy(normalizer.normalizers['observations'].stds).to(x.device).to(x.dtype).view(1, 1, -1)
+    obs_means = torch.from_numpy(normalizer.normalizers['observations'].means).to(x.device).to(x.dtype).view(1, 1, -1)
+    obs = obs * obs_stds + obs_means
+
+    # Extract relevant positions from observations
+    hand_pos = obs[:, :, :3]  # [batch_size, horizon, 3]
+    gripper_state = obs[:, :, 3:4]  # [batch_size, horizon, 1]
+    obj_pos = obs[:, :, 4:7]  # [batch_size, horizon, 3]
+    target_pos = obs[:, :, -3:]  # [batch_size, horizon, 3]
+
+    # 1. Wall avoidance loss
+    distances_to_wall = torch.abs(hand_pos - wall_pos) - wall_half_size
+    safe_distances = distances_to_wall - min_safe_dist
+    wall_penalties = torch.relu(-safe_distances) ** 2
+    wall_loss = wall_penalties.sum(dim=-1).mean(dim=1)  # [batch_size]
+
+    # 2. Grasping loss
+    hand_to_obj = torch.norm(hand_pos - obj_pos, dim=-1)  # [batch_size, horizon]
+    grasp_dist_loss = torch.exp(-5.0 * hand_to_obj) * gripper_state.squeeze(-1)
+    grasp_loss = grasp_dist_loss.mean(dim=1)  # [batch_size]
+
+    # 3. Target reaching loss
+    obj_to_target = torch.norm(obj_pos - target_pos, dim=-1)  # [batch_size, horizon]
+    target_loss = obj_to_target.mean(dim=1)  # [batch_size]
+
+    # 4. Smooth motion loss
+    action_diff = actions[:, 1:] - actions[:, :-1]
+    smoothness_loss = torch.norm(action_diff, dim=-1).mean(dim=1)  # [batch_size]
+
+    # Combine losses with weights
+    total_loss = (
+        2.0 * wall_loss +      # Strongly avoid wall collisions
+        1.0 * grasp_loss +     # Encourage grasping
+        1.5 * target_loss +    # Prioritize reaching target
+        0.1 * smoothness_loss  # Slight preference for smooth motion
+    )
+
+    if return_diagnostics:
+        diagnostics = {
+            'wall_loss': wall_loss.mean().item(),
+            'grasp_loss': grasp_loss.mean().item(),
+            'target_loss': target_loss.mean().item(),
+            'smoothness_loss': smoothness_loss.mean().item(),
+            'total_loss': total_loss.mean().item(),
+            'num_wall_violations': (wall_penalties.sum(-1) > 0).float().sum().item(),
+            'max_wall_penetration': wall_penalties.max().item(),
+            'min_obj_target_dist': obj_to_target.min().item(),
+            'min_hand_obj_dist': hand_to_obj.min().item()
+        }
+        return total_loss, diagnostics
+
+    return total_loss
+
+
 ## Initialize custom guide with your loss function
-guide = CustomGuide(loss_fn=predefined_loss_fn2, model=diffusion,normalizer=dataset.normalizer)
+guide = CustomGuide(loss_fn=pick_place_wall_loss_fn, model=diffusion,normalizer=dataset.normalizer)
 
 logger_config = utils.Config(
     utils.Logger,
@@ -364,7 +463,7 @@ speed_list = []
 pos_list = []
 d_list={}
 done=False
-for t in range(args.max_episode_length):
+for t in tqdm(range(args.max_episode_length)):
 
     if t % 10 == 0: print(args.savepath, flush=True)
 
diff --git a/scripts/plan_unguided.py b/scripts/plan_unguided.py
index 835b07c..f985cb9 100644
--- a/scripts/plan_unguided.py
+++ b/scripts/plan_unguided.py
@@ -7,6 +7,7 @@ import os
 import torch
 import numpy as np
 import json
+from tqdm import tqdm
 
 #-----------------------------------------------------------------------------#
 #----------------------------------- setup -----------------------------------#
@@ -23,8 +24,7 @@ args = Parser().parse_args('plan')
 #-----------------------------------------------------------------------------#
 
 ## load only the diffusion model from disk
-diffusion_experiment = utils.load_diffusion(
-    args.loadbase, args.dataset, args.diffusion_loadpath,
+diffusion_experiment = utils.load_diffusion(args.diffusion_loadpath,
     epoch=args.diffusion_epoch, seed=args.seed,
 )
 
@@ -75,7 +75,7 @@ for i,task in enumerate(tasks):
     speed_list = []
     pos_list = []
     success = 0
-    for t in range(args.max_episode_length):
+    for t in tqdm(range(args.max_episode_length)):
 
         if t % 10 == 0: print(args.savepath, flush=True)
 
@@ -124,7 +124,7 @@ for i,task in enumerate(tasks):
     
     success_r.append(success)
     if args.render_videos:
-        video_file = os.path.join("videos/test3_button_unguided_wall", f'trajectory_{i}_{args.dataset}_{args.horizon}.mp4')
+        video_file = os.path.join("videos/pick-place-v2", f'trajectory_{i}_{args.dataset}_{args.horizon}.mp4')
         imageio.mimwrite(video_file, frames, fps=30)
         print(f"Saved video to {video_file}")
         trajectory_data = {
@@ -157,7 +157,7 @@ for i,task in enumerate(tasks):
             print(f"Failed to save trajectory data: {e}")
 
 overall_success_rate = (sum(success_r) / len(tasks)) * 100
-with open("videos/test3_button_unguided_wall/res.txt", 'w') as f:
+with open("videos/pick-place-v2/res.txt", 'w') as f:
     f.write(f"Overall Success Rate: {overall_success_rate}%\n")
 print(f"Overall Success Rate: {overall_success_rate}%")
 ## write results to json file at `args.savepath`